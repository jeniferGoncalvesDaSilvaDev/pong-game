# ğŸ® Guia Completo: Escolhendo Algoritmos de RL para Jogos

## ğŸ“‹ Ãndice

- [IntroduÃ§Ã£o](#introduÃ§Ã£o)
- [Taxonomia de Ambientes](#taxonomia-de-ambientes)
- [Algoritmos de RL](#algoritmos-de-rl)
- [ComparaÃ§Ã£o por Jogo](#comparaÃ§Ã£o-por-jogo)
- [Matriz de DecisÃ£o](#matriz-de-decisÃ£o)
- [AnÃ¡lise de ConvergÃªncia](#anÃ¡lise-de-convergÃªncia)
- [Casos de Uso PrÃ¡ticos](#casos-de-uso-prÃ¡ticos)
- [RecomendaÃ§Ãµes](#recomendaÃ§Ãµes)

---

## ğŸ¯ IntroduÃ§Ã£o

Este documento apresenta uma anÃ¡lise comparativa de **algoritmos de Reinforcement Learning** aplicados a diferentes tipos de jogos, explicando **quando** e **por que** usar cada abordagem.

### PrincÃ­pio Fundamental

```
Complexidade do Algoritmo â‰ˆ Complexidade do Problema
```

**Lei de Occam aplicada a RL:**
> Use o algoritmo mais simples que resolva o problema adequadamente.

---

## ğŸ—‚ï¸ Taxonomia de Ambientes

### CaracterÃ­sticas dos Ambientes

| CaracterÃ­stica | DescriÃ§Ã£o | Exemplos |
|----------------|-----------|----------|
| **EspaÃ§o de estados** | Tamanho do conjunto de estados possÃ­veis | Pequeno: Tic-Tac-Toe; Grande: Go |
| **EspaÃ§o de aÃ§Ãµes** | NÃºmero de aÃ§Ãµes possÃ­veis | Discreto: Pong; ContÃ­nuo: RobÃ³tica |
| **Estacionariedade** | DinÃ¢mica do ambiente muda? | EstacionÃ¡rio: Pong; NÃ£o: Trading |
| **Observabilidade** | Agente vÃª todo estado? | Total: Xadrez; Parcial: Poker |
| **Horizonte temporal** | NÃºmero de passos atÃ© recompensa | Curto: Pong; Longo: StarCraft |
| **Determinismo** | Mesma aÃ§Ã£o â†’ mesmo resultado? | DeterminÃ­stico: Go; EstocÃ¡stico: Poker |
| **Recompensa** | Tipo de feedback | Esparsa: Xadrez; Densa: Pong |

### ClassificaÃ§Ã£o de Ambientes

#### Tipo 1: Multi-Armed Bandit (MAB)
```
âœ“ Sem estado (stateless)
âœ“ AÃ§Ã£o â†’ Recompensa imediata
âœ“ Sem transiÃ§Ãµes de estado
âœ“ EstacionÃ¡rio
```

**Exemplos:** Slot machines, Pong Bot (posiÃ§Ã£o), SeleÃ§Ã£o de anÃºncios

#### Tipo 2: MDP Simples
```
âœ“ Estados discretos (< 10â´)
âœ“ AÃ§Ãµes discretas (< 100)
âœ“ TransiÃ§Ãµes determinÃ­sticas ou pouco estocÃ¡sticas
âœ“ Horizonte curto (< 100 passos)
```

**Exemplos:** GridWorld, Frozen Lake, Tic-Tac-Toe

#### Tipo 3: MDP Complexo
```
âœ“ Estados grandes (10â´ - 10â¶)
âœ“ AÃ§Ãµes discretas ou contÃ­nuas
âœ“ TransiÃ§Ãµes complexas
âœ“ Horizonte mÃ©dio (100 - 1000 passos)
```

**Exemplos:** CartPole, LunarLander, Pong (completo)

#### Tipo 4: MDP ContÃ­nuo
```
âœ“ Estados contÃ­nuos (dimensÃ£o alta)
âœ“ AÃ§Ãµes contÃ­nuas
âœ“ FÃ­sica complexa
âœ“ Horizonte mÃ©dio-longo
```

**Exemplos:** RobÃ³tica, Controle de veÃ­culos, Humanoid

#### Tipo 5: Jogos EstratÃ©gicos
```
âœ“ Estados enormes (> 10â¶)
âœ“ Planejamento de longo prazo
âœ“ AdversÃ¡rio inteligente
âœ“ Observabilidade parcial (Ã s vezes)
```

**Exemplos:** Xadrez, Go, Poker, StarCraft

#### Tipo 6: Ambientes NÃ£o-EstacionÃ¡rios
```
âœ“ DinÃ¢mica muda ao longo do tempo
âœ“ DistribuiÃ§Ã£o de recompensas muda
âœ“ Conceito drift
```

**Exemplos:** Trading financeiro, Publicidade online, Jogos com patches

---

## ğŸ¤– Algoritmos de RL

### 1. Epsilon-Greedy (Îµ-greedy)

**Categoria:** Multi-Armed Bandit

**PseudocÃ³digo:**
```python
Initialize: Q(a) = 0, N(a) = 0 for all a
for t = 1, 2, 3, ...:
    Îµ = 1/âˆšt
    if random() < Îµ:
        a = random_action()     # ExploraÃ§Ã£o
    else:
        a = argmax_a Q(a)       # ExploitaÃ§Ã£o
    
    Observe reward r
    N(a) += 1
    Q(a) += (1/N(a)) Ã— [r - Q(a)]
```

**Complexidade:**
- Tempo: O(K) por decisÃ£o (K = # aÃ§Ãµes)
- EspaÃ§o: O(K)

**ConvergÃªncia:**
```
E[Regret] = O(log T)
```

**PrÃ³s:**
- âœ… Extremamente simples
- âœ… Sem hiperparÃ¢metros crÃ­ticos
- âœ… ConvergÃªncia garantida
- âœ… InterpretÃ¡vel

**Contras:**
- âŒ Apenas para problemas sem estado (MAB)
- âŒ NÃ£o funciona em MDPs
- âŒ ExploraÃ§Ã£o ineficiente em espaÃ§os grandes

**Quando usar:**
- EspaÃ§o de aÃ§Ãµes pequeno (< 50)
- Sem dependÃªncia temporal
- Feedback imediato
- Ambiente estacionÃ¡rio

---

### 2. Upper Confidence Bound (UCB)

**Categoria:** Multi-Armed Bandit (optimista)

**PseudocÃ³digo:**
```python
Initialize: Q(a) = 0, N(a) = 0 for all a
for t = 1, 2, 3, ...:
    UCB(a) = Q(a) + c Ã— âˆš(log(t) / N(a))
    a = argmax_a UCB(a)
    
    Observe reward r
    N(a) += 1
    Q(a) += (1/N(a)) Ã— [r - Q(a)]
```

**ParÃ¢metro:** `c` (tipicamente `c = âˆš2`)

**ConvergÃªncia:**
```
E[Regret] = O(log T)   (mas com constantes menores que Îµ-greedy)
```

**PrÃ³s:**
- âœ… Melhor que Îµ-greedy em regret
- âœ… NÃ£o precisa ajustar Îµ
- âœ… ExploraÃ§Ã£o inteligente (otimismo)

**Contras:**
- âŒ Apenas para MAB
- âŒ Mais complexo que Îµ-greedy
- âŒ SensÃ­vel ao parÃ¢metro `c`

**Quando usar:**
- Quando Îµ-greedy seria adequado, mas vocÃª quer menor regret
- EspaÃ§o pequeno de aÃ§Ãµes
- Recompensas com distribuiÃ§Ãµes diferentes

---

### 3. Thompson Sampling

**Categoria:** Multi-Armed Bandit (Bayesiano)

**PseudocÃ³digo:**
```python
Initialize: Î±(a) = 1, Î²(a) = 1 for all a  # Beta priors
for t = 1, 2, 3, ...:
    for each action a:
        Î¸(a) ~ Beta(Î±(a), Î²(a))
    
    a = argmax_a Î¸(a)
    
    Observe reward r âˆˆ {0, 1}
    if r == 1:
        Î±(a) += 1
    else:
        Î²(a) += 1
```

**ConvergÃªncia:**
```
E[Regret] = O(log T)
```

**PrÃ³s:**
- âœ… ExploraÃ§Ã£o probabilÃ­stica natural
- âœ… Muito eficiente na prÃ¡tica
- âœ… Funciona bem com recompensas binÃ¡rias
- âœ… Sem hiperparÃ¢metros

**Contras:**
- âŒ Apenas para MAB
- âŒ Requer conhecimento de distribuiÃ§Ãµes
- âŒ Mais complexo de implementar

**Quando usar:**
- Recompensas binÃ¡rias ou Gaussianas
- Quando tem prior sobre distribuiÃ§Ãµes
- Quer melhor performance que UCB

---

### 4. Q-Learning

**Categoria:** Value-based, Off-policy, Tabular

**EquaÃ§Ã£o de Bellman:**
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
```

**PseudocÃ³digo:**
```python
Initialize: Q(s,a) = 0 for all s, a
for episode = 1, 2, ...:
    s = initial_state()
    while not done:
        a = Îµ-greedy(Q[s])
        s', r, done = env.step(a)
        
        Q[s,a] += Î± Ã— [r + Î³ Ã— max_a' Q[s',a'] - Q[s,a]]
        s = s'
```

**Complexidade:**
- Tempo: O(|S| Ã— |A|) por episÃ³dio
- EspaÃ§o: O(|S| Ã— |A|)

**ConvergÃªncia:**
```
Q(s,a) â†’ Q*(s,a)  quando t â†’ âˆ
(sob condiÃ§Ãµes de Robbins-Monro)
```

**PrÃ³s:**
- âœ… Funciona em MDPs
- âœ… Off-policy (aprende polÃ­tica Ã³tima independente da exploraÃ§Ã£o)
- âœ… Garantia de convergÃªncia
- âœ… Simples de implementar

**Contras:**
- âŒ Apenas estados discretos pequenos
- âŒ NÃ£o escala (curse of dimensionality)
- âŒ ConvergÃªncia lenta em espaÃ§os grandes

**Quando usar:**
- Estados discretos (< 10â´)
- AÃ§Ãµes discretas (< 100)
- TransiÃ§Ãµes estocÃ¡sticas
- Horizonte curto-mÃ©dio

---

### 5. SARSA

**Categoria:** Value-based, On-policy, Tabular

**EquaÃ§Ã£o:**
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
```

**DiferenÃ§a de Q-Learning:**
- Q-Learning: usa `max_a' Q(s',a')` (polÃ­tica Ã³tima)
- SARSA: usa `Q(s',a')` onde `a'` foi realmente tomada (polÃ­tica atual)

**PrÃ³s:**
- âœ… Mais conservador que Q-Learning
- âœ… Melhor em ambientes com penalidades altas
- âœ… Considera exploraÃ§Ã£o na atualizaÃ§Ã£o

**Contras:**
- âŒ Mesmas limitaÃ§Ãµes de Q-Learning
- âŒ ConvergÃªncia mais lenta

**Quando usar:**
- Ambientes perigosos (cliff walking)
- Quando polÃ­tica de exploraÃ§Ã£o importa
- Precisa de comportamento conservador

---

### 6. Deep Q-Network (DQN)

**Categoria:** Value-based, Off-policy, Deep Learning

**InovaÃ§Ãµes:**
1. **Experience Replay:** Buffer de transiÃ§Ãµes (s,a,r,s')
2. **Target Network:** Rede separada para targets estÃ¡veis
3. **Approximador:** Rede neural ao invÃ©s de tabela

**PseudocÃ³digo:**
```python
Initialize: Q_Î¸ (online), Q_Î¸' (target), Replay Buffer D
for episode = 1, 2, ...:
    s = initial_state()
    while not done:
        a = Îµ-greedy(Q_Î¸[s])
        s', r, done = env.step(a)
        D.store(s, a, r, s', done)
        
        # Sample minibatch
        batch = D.sample(batch_size)
        
        # Compute targets
        y = r + Î³ Ã— max_a' Q_Î¸'[s',a']
        
        # Update Q-network
        loss = (y - Q_Î¸[s,a])Â²
        Î¸ â† Î¸ - Î±âˆ‡loss
        
        # Update target network (periodically)
        if t % C == 0:
            Î¸' â† Î¸
        
        s = s'
```

**Complexidade:**
- Tempo: O(batch_size Ã— forward_pass)
- EspaÃ§o: O(|D| + |Î¸|)

**PrÃ³s:**
- âœ… Funciona em espaÃ§os contÃ­nuos de estados
- âœ… Pode processar imagens (CNN)
- âœ… Melhor que tabular em estados grandes
- âœ… Compartilha features entre estados

**Contras:**
- âŒ Apenas aÃ§Ãµes discretas
- âŒ InstÃ¡vel (requer tricks)
- âŒ Hiper-sensÃ­vel a hiperparÃ¢metros
- âŒ Sample inefficient

**Quando usar:**
- Estados contÃ­nuos ou alta dimensÃ£o
- AÃ§Ãµes discretas
- Tem muitos dados (milhÃµes de frames)
- Pode treinar por dias

---

### 7. Policy Gradient (REINFORCE)

**Categoria:** Policy-based, On-policy

**Objetivo:**
```
J(Î¸) = E_Ï€[âˆ‘ Î³^t r_t]
```

**Gradiente:**
```
âˆ‡J(Î¸) = E_Ï€[âˆ‡log Ï€_Î¸(a|s) Ã— Q^Ï€(s,a)]
```

**PseudocÃ³digo:**
```python
Initialize: Ï€_Î¸ (policy network)
for episode = 1, 2, ...:
    Ï„ = sample_trajectory(Ï€_Î¸)  # (s_0,a_0,r_0,...,s_T)
    
    # Compute returns
    G_t = âˆ‘_{t'=t}^T Î³^(t'-t) Ã— r_t'
    
    # Update policy
    for t in range(T):
        Î¸ â† Î¸ + Î± Ã— âˆ‡log Ï€_Î¸(a_t|s_t) Ã— G_t
```

**PrÃ³s:**
- âœ… Funciona com aÃ§Ãµes contÃ­nuas
- âœ… Pode aprender polÃ­ticas estocÃ¡sticas
- âœ… Garante convergÃªncia (local mÃ­nima)
- âœ… Melhor para alta dimensÃ£o de aÃ§Ãµes

**Contras:**
- âŒ Alta variÃ¢ncia
- âŒ Sample inefficient
- âŒ ConvergÃªncia lenta
- âŒ SensÃ­vel a learning rate

**Quando usar:**
- AÃ§Ãµes contÃ­nuas
- PolÃ­tica estocÃ¡stica necessÃ¡ria
- EspaÃ§o de aÃ§Ãµes muito grande

---

### 8. Actor-Critic (A2C/A3C)

**Categoria:** HÃ­brido (Policy + Value)

**Componentes:**
1. **Actor:** PolÃ­tica Ï€_Î¸(a|s)
2. **Critic:** FunÃ§Ã£o valor V_Ï†(s) ou Q_Ï†(s,a)

**Vantagem:**
```
A(s,a) = Q(s,a) - V(s) = r + Î³V(s') - V(s)
```

**PseudocÃ³digo:**
```python
Initialize: Ï€_Î¸ (actor), V_Ï† (critic)
for episode = 1, 2, ...:
    s = initial_state()
    while not done:
        a ~ Ï€_Î¸(Â·|s)
        s', r, done = env.step(a)
        
        # TD error (advantage)
        Î´ = r + Î³V_Ï†(s') - V_Ï†(s)
        
        # Update critic
        Ï† â† Ï† - Î±_v Ã— âˆ‡(Î´Â²)
        
        # Update actor
        Î¸ â† Î¸ + Î±_Ï€ Ã— âˆ‡log Ï€_Î¸(a|s) Ã— Î´
        
        s = s'
```

**PrÃ³s:**
- âœ… Menor variÃ¢ncia que REINFORCE
- âœ… AÃ§Ãµes contÃ­nuas ou discretas
- âœ… Mais sample efficient que PG puro
- âœ… Online learning

**Contras:**
- âŒ Duas redes para treinar
- âŒ Mais hiperparÃ¢metros
- âŒ Pode ser instÃ¡vel

**Quando usar:**
- Quer vantagens de PG com menor variÃ¢ncia
- AÃ§Ãµes contÃ­nuas ou discretas
- Tem recursos computacionais

---

### 9. Proximal Policy Optimization (PPO)

**Categoria:** Policy-based, On-policy, State-of-the-art

**Objetivo clipped:**
```
L^CLIP(Î¸) = E[min(r_t(Î¸)Ã—A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Ã—A_t)]
```

**Onde:**
```
r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)
```

**PrÃ³s:**
- âœ… SOTA em muitos benchmarks
- âœ… EstÃ¡vel (limita updates)
- âœ… AÃ§Ãµes contÃ­nuas ou discretas
- âœ… Relativamente robusto

**Contras:**
- âŒ Complexo de implementar
- âŒ Muitos hiperparÃ¢metros
- âŒ On-policy (menos sample efficient)

**Quando usar:**
- Problemas complexos de controle
- Tem recursos computacionais
- Quer estado-da-arte

---

### 10. Soft Actor-Critic (SAC)

**Categoria:** Off-policy, Maximum entropy RL

**Objetivo:**
```
J(Ï€) = âˆ‘_t E[(r_t + Î± H(Ï€(Â·|s_t)))]
```

**Entropia:** `H(Ï€) = -E[log Ï€(a|s)]`

**PrÃ³s:**
- âœ… Sample efficient (off-policy)
- âœ… Muito estÃ¡vel
- âœ… AÃ§Ãµes contÃ­nuas
- âœ… ExploraÃ§Ã£o automÃ¡tica (entropia)

**Contras:**
- âŒ Muito complexo
- âŒ Apenas aÃ§Ãµes contÃ­nuas
- âŒ Custoso computacionalmente

**Quando usar:**
- RobÃ³tica
- Controle contÃ­nuo
- Precisa de estabilidade

---

### 11. AlphaZero / MCTS

**Categoria:** Planning, Tree Search

**Componentes:**
1. **Monte Carlo Tree Search (MCTS)**
2. **Neural Network:** (polÃ­tica + valor)
3. **Self-play**

**PseudocÃ³digo MCTS:**
```python
def mcts(root_state, num_simulations):
    tree = {root_state: Node()}
    
    for _ in range(num_simulations):
        node = root_state
        path = [node]
        
        # 1. Selection (UCB)
        while not is_leaf(node):
            node = select_child(node, UCB_score)
            path.append(node)
        
        # 2. Expansion
        if not terminal(node):
            expand(node)
            node = random_child(node)
            path.append(node)
        
        # 3. Simulation
        value = rollout(node)
        
        # 4. Backpropagation
        for n in reversed(path):
            update_stats(n, value)
            value = -value  # adversarial
    
    return best_action(root_state)
```

**PrÃ³s:**
- âœ… Melhor para jogos estratÃ©gicos
- âœ… Planejamento explÃ­cito
- âœ… Anytime algorithm
- âœ… Encontra soluÃ§Ãµes Ã³timas

**Contras:**
- âŒ Computacionalmente caro
- âŒ Precisa de simulador
- âŒ NÃ£o funciona em tempo real
- âŒ Apenas ambientes determinÃ­sticos

**Quando usar:**
- Jogos de tabuleiro (Xadrez, Go)
- Planejamento perfeito necessÃ¡rio
- Tem tempo para computaÃ§Ã£o

---

## ğŸ® ComparaÃ§Ã£o por Jogo

### 1. Pong (Bot - apenas posiÃ§Ã£o)

**CaracterÃ­sticas do ambiente:**
```
Tipo: Multi-Armed Bandit
Estados: Stateless (apenas Y da bola)
AÃ§Ãµes: 10 posiÃ§Ãµes discretas
Horizonte: 1 passo
Recompensas: Densas, imediatas
EstacionÃ¡rio: Sim
```

**Algoritmos aplicÃ¡veis:**

| Algoritmo | Score | Justificativa |
|-----------|-------|---------------|
| **Îµ-greedy** | â­â­â­â­â­ | PERFEITO - MAB clÃ¡ssico, converge em ~20 tentativas |
| **UCB** | â­â­â­â­ | Bom - Menor regret que Îµ-greedy, mas overhead desnecessÃ¡rio |
| **Thompson** | â­â­â­â­ | Bom - Eficiente, mas complexo demais para o problema |
| **Q-Learning** | â­â­ | OVERKILL - NÃ£o hÃ¡ estado, framework MDP desnecessÃ¡rio |
| **DQN** | â­ | ABSURDO - Como usar canhÃ£o para matar mosca |
| **PPO** | â­ | ABSURDO - Complexidade injustificada |

**Melhor escolha:** âœ… **Îµ-greedy**

**ConvergÃªncia esperada:**
```
Tentativas:  10    20    30    50
PrecisÃ£o:    60%   80%   90%   95%
```

**CÃ³digo de referÃªncia:** Veja implementaÃ§Ã£o acima no jogo.

---

### 2. Pong (Completo - jogo inteiro)

**CaracterÃ­sticas do ambiente:**
```
Tipo: MDP Complexo
Estados: (x_bola, y_bola, vx, vy, y_jogador, y_bot)
AÃ§Ãµes: 3 (cima, baixo, parado)
Horizonte: ~100 passos por episÃ³dio
Recompensas: Esparsas (+1/-1 no gol)
```

**Algoritmos aplicÃ¡veis:**

| Algoritmo | Score | Justificativa |
|-----------|-------|---------------|
| **Q-Learning** | â­â­â­ | OK - Se discretizar estados, funciona mas lento |
| **DQN** | â­â­â­â­â­ | EXCELENTE - Benchmark clÃ¡ssico, converge bem |
| **Policy Gradient** | â­â­â­ | OK - Funciona mas alta variÃ¢ncia |
| **A3C** | â­â­â­â­ | Bom - Mais estÃ¡vel que PG, paralelizÃ¡vel |
| **PPO** | â­â­â­â­ | Bom - SOTA mas overkill para Pong |

**Melhor escolha:** âœ… **DQN** (clÃ¡ssico) ou **A3C** (moderno)

**Dificuldades:**
- Recompensas esparsas â†’ Precisa de muitos episÃ³dios
- ExploraÃ§Ã£o inicial ruim â†’ Nunca marca ponto = nÃ£o aprende
- SoluÃ§Ã£o: **Curriculum learning** ou **reward shaping**

---

### 3. CartPole

**CaracterÃ­sticas:**
```
Tipo: MDP Simples
Estados: (x, áº‹, Î¸, Î¸Ì‡) - 4D contÃ­nuo
AÃ§Ãµes: 2 (esquerda, direita)
Horizonte: ~200 passos
Recompensas: +1 por passo
```

**Algoritmos aplicÃ¡veis:**

| Algoritmo | Score | Justificativa |
|-----------|-------|---------------|
| **Q-Learning** | â­â­ | Ruim - Precisa discretizar estados contÃ­nuos |
| **DQN** | â­â­â­â­ | Bom - Funciona bem, benchmark comum |
| **Policy Gradient** | â­â­â­â­ | Bom - Alta variÃ¢ncia mas resolve |
| **A3C** | â­â­â­â­â­ | EXCELENTE - Baixa variÃ¢ncia, rÃ¡pido |
| **PPO** | â­â­â­â­â­ | EXCELENTE - SOTA, muito estÃ¡vel |

**Melhor escolha:** âœ… **PPO** ou **A3C**

**ConvergÃªncia:**
```
DQN:  ~500 episÃ³dios
A3C:  ~300 episÃ³dios
PPO:  ~200 episÃ³dios
```

---

### 4. Atari Games (ex: Breakout)

**CaracterÃ­sticas:**
```
Tipo: MDP Complexo, Visual
Estados: 84Ã—84Ã—4 pixels (frames)
AÃ§Ãµes: 4-18 (depende do jogo)
Horizonte: ~1000 frames
Recompensas: VariÃ¡vel (jogo-dependente)
```

**Algoritmos aplicÃ¡veis:**

| Algoritmo | Score | Justificativa |
|-----------|-------|---------------|
| **DQN** | â­â­â­â­â­ | LANDMARK - Paper original, funciona |
| **Double DQN** | â­â­â­â­â­ | Melhor - Corrige overestimation |
| **Rainbow DQN** | â­â­â­â­â­ | SOTA (value) - Combina 6 melhorias |
| **A3C** | â­â­â­â­ | Bom - ParalelizaÃ§Ã£o importante |
| **PPO** | â­â­â­â­ | Bom - Mais sample efficient que A3C |
| **MuZero** | â­â­â­â­â­ | SOTA - Aprende modelo + planning |

**Melhor escolha:** âœ… **Rainbow DQN** (value-based) ou **PPO** (policy-based)

**Desafios:**
- MilhÃµes de frames necessÃ¡rios
- Dias/semanas de treinamento
- Precisa de GPUs potentes

---

### 5. LunarLander

**CaracterÃ­sticas:**
```
Tipo: MDP Complexo, ContÃ­nuo
Estados: 8D (x, y, vx, vy, Î¸, Ï‰, leg1, leg2)
AÃ§Ãµes: 4 discretas OU contÃ­nuas
Horizonte: ~300 passos
Recompensas: Shaped (distÃ¢ncia, velocidade, pouso)
```

**Algoritmos aplicÃ¡veis:**

| Algoritmo | Score | Justificativa |
|-----------|-------|---------------|
| **DQN** | â­â­â­â­ | Bom - VersÃ£o discreta funciona bem |
| **PPO** | â­â­â­â­â­ | EXCELENTE - EstÃ¡vel, versÃ£o contÃ­nua |
| **SAC** | â­â­â­â­â­ | EXCELENTE - AÃ§Ãµes contÃ­nuas, sample efficient |
| **TD3** | â­â­â­â­ | Bom - AÃ§Ãµes contÃ­nuas, menos variÃ¢ncia |

**Melhor escolha:** 
- AÃ§Ãµes discretas: âœ… **DQN** ou **PPO**
- AÃ§Ãµes contÃ­nuas: âœ… **SAC** ou **TD3**

---

### 6. Xadrez

**CaracterÃ­sticas:**
```
Tipo: Jogo EstratÃ©gico, Adversarial
Estados: ~10â´â° posiÃ§Ãµes legais
AÃ§Ãµes: ~35 mÃ©dias por posiÃ§Ã£o
Horizonte: ~40 movimentos (80 plies)
Recompensas: {-1, 0, +1} no final
DeterminÃ­stico: Sim
```

**Algoritmos aplicÃ¡veis:**

| Algoritmo | Score | Justificativa |
|-----------|-------|---------------|
| **Minimax + Î±Î²** | â­â­â­â­ | ClÃ¡ssico - Stockfish usa (nÃ£o RL) |
| **MCTS** | â­â­â­ | OK - Sem heurÃ­stica Ã© fraco |
| **AlphaZero** | â­â­â­â­â­ | SOTA - Superhuman, self-play |
| **MuZero** | â­â­â­â­â­ | SOTA - Aprende regras tambÃ©m |
| **DQN/PPO** | â­ | PÃ‰SSIMO - EspaÃ§o de aÃ§Ãµes gigante |

**Melhor escolha:** âœ… **AlphaZero** (RL) ou **Stockfish** (clÃ¡ssico)

**Por que RL "simples" falha:**
- EspaÃ§o de aÃ§Ãµes enorme (~35 por estado)
- Horizonte longo (decisÃµes afetam 20+ movimentos)
- Recompensa apenas no final (sparse)
- Precisa de planejamento profundo

**AlphaZero solve isso com:**
1. MCTS para planejamento
2. Rede neural para heurÃ­stica
3. Self-play para dados
4. MilhÃµes de jogos

---

### 7. Go

**CaracterÃ­sticas:**
```
Tipo: Jogo EstratÃ©gico, Adversarial
Estados: ~10Â¹â·â° posiÃ§Ãµes legais
AÃ§Ãµes: ~250 mÃ©dias (19Ã—19 = 361 possÃ­veis)
Horizonte: ~150 movimentos
Recompensas: {-1, 0, +1} no final
```

**Algoritmos aplicÃ¡veis:**

| Algoritmo | Score | Justificativa |
|-----------|-------|---------------|
| **MCTS** | â­â­â­ | OK - Melhor que minimax em Go |
| **AlphaGo** | â­â­â­â­â­ | MARCO - Venceu humanos |
| **AlphaGo Zero** | â­â­â­â­â­ | Melhor - Self-play puro |
| **KataGo** | â­â­â­â­





7. Go (continuaÃ§Ã£o)
Algoritmos aplicÃ¡veis:
Algoritmo
Score
Justificativa
MCTS puro
â­â­â­
Funciona melhor que minimax, mas fraco sem boa heurÃ­stica
AlphaGo
â­â­â­â­â­
Marco histÃ³rico â€” policy + value + self-play
AlphaGo Zero
â­â­â­â­â­
Self-play puro, supera AlphaGo original
KataGo
â­â­â­â­â­
SOTA atual â€” melhor eficiÃªncia, mÃºltiplos objetivos
DQN / PPO
â­
InviÃ¡vel â€” espaÃ§o de aÃ§Ãµes e horizonte gigantes
Por que KataGo Ã© superior hoje?
KataGo melhora AlphaZero em vÃ¡rios pontos:
ğŸ¯ Treino multi-objetivo (nÃ£o sÃ³ win/loss)
ğŸ§  Melhor uso de dados (menos partidas que AlphaZero)
âš™ï¸ Arquitetura refinada
ğŸ“‰ Menor custo computacional
Melhor escolha: âœ… KataGo (estado da arte atual em Go)
ğŸ“Š Matriz de DecisÃ£o (Resumo Geral)
Escolha rÃ¡pida do algoritmo
Tipo de Ambiente
Melhor Algoritmo
Alternativa
MAB simples
Îµ-greedy
UCB
MAB com prior
Thompson Sampling
UCB
MDP pequeno (discreto)
Q-Learning
SARSA
MDP mÃ©dio (discreto)
DQN
Double DQN
Estados contÃ­nuos
PPO
A2C
AÃ§Ãµes contÃ­nuas
SAC
TD3
Visual (Atari)
Rainbow DQN
PPO
EstratÃ©gico (tabuleiro)
AlphaZero
MCTS
NÃ£o-estacionÃ¡rio
Bandits adaptativos
Meta-RL
ğŸ“‰ AnÃ¡lise de ConvergÃªncia (Comparativa)
Velocidade vs Estabilidade
Algoritmo
Velocidade
Estabilidade
Sample Efficiency
Îµ-greedy
â­â­â­â­â­
â­â­â­â­â­
â­â­â­â­
UCB
â­â­â­â­
â­â­â­â­â­
â­â­â­â­
Q-Learning
â­â­â­
â­â­â­â­
â­â­
SARSA
â­â­
â­â­â­â­â­
â­â­
DQN
â­â­â­
â­â­
â­
A3C
â­â­â­â­
â­â­â­
â­â­
PPO
â­â­â­â­
â­â­â­â­
â­â­â­
SAC
â­â­â­â­
â­â­â­â­â­
â­â­â­â­
AlphaZero
â­
â­â­â­â­â­
â­
Regra prÃ¡tica de convergÃªncia
Copiar cÃ³digo

Ambiente simples  â†’ Algoritmo simples
Ambiente complexo â†’ Planejamento + Deep RL
ğŸ§ª Casos de Uso PrÃ¡ticos
ğŸ® Jogos Indie / SimulaÃ§Ãµes simples
GridWorld, Pong simples
âœ… Îµ-greedy, Q-learning
ğŸ¤– SimulaÃ§Ãµes fÃ­sicas
CartPole, LunarLander
âœ… PPO, SAC
ğŸ•¹ï¸ Jogos visuais
Atari, plataformas
âœ… DQN, PPO
â™Ÿï¸ Jogos estratÃ©gicos
Xadrez, Go
âœ… AlphaZero, KataGo
ğŸ’¹ Ambientes mutÃ¡veis
Trading, jogos com patch
âœ… Thompson Sampling, Meta-RL
ğŸ§  RecomendaÃ§Ãµes Finais (A Parte Mais Importante)
1ï¸âƒ£ Nunca comece com Deep RL
80% dos problemas em jogos nÃ£o precisam de redes neurais.
Comece sempre com:
Bandits
Q-learning
SARSA
Depois escale.
2ï¸âƒ£ Se vocÃª consegue desenhar a tabela Q, use Q-learning
Se |S| Ã— |A| < 10â¶, deep RL Ã© desperdÃ­cio.
3ï¸âƒ£ Planejamento vence aprendizado puro
Em jogos estratÃ©gicos:
AlphaZero > PPO
MCTS > DQN
Porque pensar > reagir.
4ï¸âƒ£ ExploraÃ§Ã£o Ã© o verdadeiro problema
A maioria dos fracassos em RL vem de:
Recompensa esparsa
ExploraÃ§Ã£o ruim
Use:
Reward shaping
Curriculum learning
Entropy (SAC / PPO)
5ï¸âƒ£ Lei de Ouro do RL em Jogos
Copiar cÃ³digo

Se seu agente nÃ£o aprende:
â†’ O problema NÃƒO Ã© o algoritmo
â†’ Ã‰ o ambiente, recompensa ou observaÃ§Ã£o
ğŸ ConclusÃ£o
Este guia fornece um mapa mental completo para escolher algoritmos de RL em jogos:
NÃ£o existe algoritmo universal
Complexidade deve ser justificada
Planejamento + RL vence forÃ§a bruta
Simplicidade Ã© poder
Se quiser, posso:
ğŸ¯ Criar um fluxograma visual de decisÃ£o
ğŸ§© Montar roadmap prÃ¡tico por nÃ­vel
ğŸ•¹ï¸ Aplicar isso diretamente no seu jogo em Phaser
ğŸ¤– Ajudar a projetar o ambiente certo para RL
Ã‰ sÃ³ dizer como quer continuar.
