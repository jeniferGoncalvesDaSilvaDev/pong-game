# ğŸ® Pong com Epsilon-Greedy Reinforcement Learning

![Version](https://img.shields.io/badge/version-2.0.0-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![AI](https://img.shields.io/badge/AI-Epsilon--Greedy-orange)
![RL](https://img.shields.io/badge/RL-Multi--Armed_Bandit-purple)

> ImplementaÃ§Ã£o do clÃ¡ssico jogo Pong com IA baseada em **Epsilon-Greedy Multi-Armed Bandit** para aprendizado por reforÃ§o online.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Epsilon-Greedy Algorithm](#epsilon-greedy-algorithm)
- [EquaÃ§Ãµes MatemÃ¡ticas](#equaÃ§Ãµes-matemÃ¡ticas)
- [MÃ©tricas da IA](#mÃ©tricas-da-ia)
- [ImplementaÃ§Ã£o](#implementaÃ§Ã£o)
- [AnÃ¡lise de ConvergÃªncia](#anÃ¡lise-de-convergÃªncia)
- [Experimentos](#experimentos)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [Como Usar](#como-usar)

---

## ğŸ¯ VisÃ£o Geral

Este projeto implementa um **Multi-Armed Bandit (MAB)** usando o algoritmo **Epsilon-Greedy** para controlar a raquete do bot no jogo Pong. A IA aprende online qual a melhor posiÃ§Ã£o para interceptar a bola atravÃ©s de tentativa e erro.

### Por que Epsilon-Greedy para Pong?

1. **EspaÃ§o de estados pequeno** - 10 posiÃ§Ãµes discretas (bins)
2. **Feedback imediato** - recompensa apÃ³s cada tentativa
3. **Ambiente estacionÃ¡rio** - dinÃ¢mica do jogo nÃ£o muda
4. **ConvergÃªncia rÃ¡pida** - aprende a polÃ­tica Ã³tima em ~20-30 tentativas
5. **Interpretabilidade** - mÃ©tricas claras e visualizÃ¡veis

### CaracterÃ­sticas

- âœ… Aprendizado online (sem treinamento prÃ©vio)
- âœ… Decaimento automÃ¡tico de epsilon
- âœ… MÃ©tricas detalhadas em tempo real
- âœ… AnÃ¡lise estatÃ­stica completa
- âœ… VisualizaÃ§Ã£o de Q-values
- âœ… CÃ¡lculo de regret

---

## ğŸ§  Epsilon-Greedy Algorithm

### Conceito Fundamental

O algoritmo **Epsilon-Greedy** resolve o dilema **exploraÃ§Ã£o vs exploitaÃ§Ã£o** em problemas de decisÃ£o sequencial:

- **ExploraÃ§Ã£o (Îµ):** Testar aÃ§Ãµes aleatÃ³rias para descobrir recompensas
- **ExploitaÃ§Ã£o (1-Îµ):** Escolher a melhor aÃ§Ã£o conhecida atÃ© o momento

### PseudocÃ³digo

```
Inicializar:
  Para cada aÃ§Ã£o a âˆˆ {0, 1, ..., 9}:
    Q(a) â† 0        // Valor estimado
    N(a) â† 0        // Contador de tentativas
  t â† 0             // Tentativa atual

Loop (a cada decisÃ£o):
  t â† t + 1
  Îµ â† 1/âˆšt          // Decaimento de epsilon
  
  Com probabilidade Îµ:
    a â† aÃ§Ã£o aleatÃ³ria                    // EXPLORAÃ‡ÃƒO
  Caso contrÃ¡rio:
    a â† argmax_a Q(a)                     // EXPLOITAÃ‡ÃƒO
  
  Executar aÃ§Ã£o a, observar recompensa r
  
  N(a) â† N(a) + 1
  Î± â† 1/N(a)                              // Learning rate
  Q(a) â† Q(a) + Î± Ã— [r - Q(a)]           // AtualizaÃ§Ã£o incremental
```

### DiscretizaÃ§Ã£o do EspaÃ§o de Estados

O espaÃ§o contÃ­nuo de posiÃ§Ãµes Y [0, 600] Ã© discretizado em **10 bins**:

```
Bin 0: Y âˆˆ [  0,  60) â†’ AÃ§Ã£o 0 â†’ Alvo Y = 30
Bin 1: Y âˆˆ [ 60, 120) â†’ AÃ§Ã£o 1 â†’ Alvo Y = 90
Bin 2: Y âˆˆ [120, 180) â†’ AÃ§Ã£o 2 â†’ Alvo Y = 150
Bin 3: Y âˆˆ [180, 240) â†’ AÃ§Ã£o 3 â†’ Alvo Y = 210
Bin 4: Y âˆˆ [240, 300) â†’ AÃ§Ã£o 4 â†’ Alvo Y = 270  â­ CENTRO
Bin 5: Y âˆˆ [300, 360) â†’ AÃ§Ã£o 5 â†’ Alvo Y = 330
Bin 6: Y âˆˆ [360, 420) â†’ AÃ§Ã£o 6 â†’ Alvo Y = 390
Bin 7: Y âˆˆ [420, 480) â†’ AÃ§Ã£o 7 â†’ Alvo Y = 450
Bin 8: Y âˆˆ [480, 540) â†’ AÃ§Ã£o 8 â†’ Alvo Y = 510
Bin 9: Y âˆˆ [540, 600] â†’ AÃ§Ã£o 9 â†’ Alvo Y = 570
```

**FunÃ§Ã£o de discretizaÃ§Ã£o:**
```
bin(y) = min(9, âŒŠy / 60âŒ‹)
```

---

## ğŸ“ EquaÃ§Ãµes MatemÃ¡ticas

### 1. Decaimento de Epsilon

**FÃ³rmula:**
```
Îµ(t) = 1/âˆš(t + 1)
```

**CaracterÃ­sticas:**
- `Îµ(0) = 1.0` â†’ 100% exploraÃ§Ã£o inicial
- `Îµ(10) â‰ˆ 0.302` â†’ 30% exploraÃ§Ã£o
- `Îµ(100) â‰ˆ 0.099` â†’ 10% exploraÃ§Ã£o
- `Îµ(âˆ) â†’ 0` â†’ converge para exploitaÃ§Ã£o pura

**GrÃ¡fico de decaimento:**
```
Îµ â”‚
1.0â”œâ”€â•®
   â”‚  â•²
0.5â”‚   â•²___
   â”‚       â•²____
0.1â”‚            â•²_______
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ t
   0   10   50  100  500
```

**Por que âˆšt?**
- Decai mais rÃ¡pido que `1/t` (permite convergÃªncia)
- Decai mais devagar que `1/tÂ²` (permite exploraÃ§Ã£o suficiente)
- Garante `âˆ‘Îµ(t) = âˆ` e `âˆ‘ÎµÂ²(t) < âˆ` (condiÃ§Ãµes de convergÃªncia)

### 2. AtualizaÃ§Ã£o de Q-values (Sample Average)

**FÃ³rmula incremental:**
```
Q_n+1(a) = Q_n(a) + Î± Ã— [R_n - Q_n(a)]
```

**Onde:**
- `Q_n(a)` = valor estimado da aÃ§Ã£o `a` apÃ³s `n` observaÃ§Ãµes
- `R_n` = recompensa observada na tentativa `n`
- `Î± = 1/n` = learning rate (taxa de aprendizado)

**Expandindo:**
```
Q_n+1(a) = Q_n(a) + (1/n) Ã— [R_n - Q_n(a)]
         = (1/n) Ã— R_n + (1 - 1/n) Ã— Q_n(a)
         = (1/n) Ã— R_n + ((n-1)/n) Ã— Q_n(a)
```

**Forma recursiva:**
```
Q_1(a) = R_1
Q_2(a) = (R_1 + R_2) / 2
Q_3(a) = (R_1 + R_2 + R_3) / 3
...
Q_n(a) = (1/n) Ã— âˆ‘_{i=1}^{n} R_i
```

### 3. FunÃ§Ã£o de Recompensa

**Recompensa por acertar a bola:**
```
r_hit(d) = 1 / (1 + d/50)
```

**Onde:**
- `d = |y_bola - y_raquete|` = distÃ¢ncia euclidiana
- `d = 0` â†’ `r = 1.0` (perfeito)
- `d = 50` â†’ `r = 0.5` (mÃ©dio)
- `d = 100` â†’ `r = 0.33` (ruim)

**Recompensa por perder ponto:**
```
r_miss = -1.0
```

**CaracterÃ­sticas:**
- Recompensa contÃ­nua proporcional Ã  qualidade da interceptaÃ§Ã£o
- Penalidade fixa por falha
- Incentiva posicionamento preciso

### 4. Regret (Arrependimento)

**Regret instantÃ¢neo:**
```
â„“_t = Q*(a*) - Q*(a_t)
```

**Regret acumulado:**
```
L_T = âˆ‘_{t=1}^{T} â„“_t = âˆ‘_{t=1}^{T} [Q*(a*) - Q*(a_t)]
```

**Regret mÃ©dio:**
```
LÌ„_T = L_T / T
```

**Onde:**
- `a*` = aÃ§Ã£o Ã³tima (desconhecida a priori)
- `a_t` = aÃ§Ã£o escolhida no tempo `t`
- `Q*(a)` = valor verdadeiro da aÃ§Ã£o `a`

**Limite teÃ³rico do Epsilon-Greedy:**
```
E[L_T] = O(log T)
```

### 5. Probabilidade de ExploraÃ§Ã£o/ExploitaÃ§Ã£o

**DistribuiÃ§Ã£o Binomial:**

A escolha entre exploraÃ§Ã£o e exploitaÃ§Ã£o segue uma distribuiÃ§Ã£o de Bernoulli:
```
X_t ~ Bernoulli(1 - Îµ(t))
```

Onde `X_t = 1` se exploita, `X_t = 0` se explora.

**ApÃ³s T tentativas:**
```
Y_T = âˆ‘_{t=1}^{T} X_t ~ Binomial(T, pÌ„)
```

**ParÃ¢metros:**
- `n = T` = nÃºmero de tentativas
- `pÌ„ = (âˆ‘Îµ(t)) / T` = probabilidade mÃ©dia de exploitaÃ§Ã£o

**EstatÃ­sticas:**
```
E[Y_T] = T Ã— pÌ„                    // MÃ©dia
Var[Y_T] = T Ã— pÌ„ Ã— (1 - pÌ„)        // VariÃ¢ncia
Ïƒ[Y_T] = âˆš(T Ã— pÌ„ Ã— (1 - pÌ„))       // Desvio padrÃ£o
```

### 6. Recompensa Esperada

**Valor esperado baseado na polÃ­tica atual:**
```
E[R] = âˆ‘_{a=0}^{9} Ï€(a) Ã— Q(a)
```

**Onde:**
```
Ï€(a) = P(selecionar aÃ§Ã£o a) = N(a) / T
```

**Recompensa esperada Ã³tima (limite superior):**
```
E[R*] = max_a Q(a)
```

### 7. Taxa de ConvergÃªncia

**Probabilidade de escolher aÃ§Ã£o Ã³tima:**
```
P(a_t = a*) â‰¥ 1 - Îµ(t) = 1 - 1/âˆš(t+1)
```

**ConvergÃªncia assintÃ³tica:**
```
lim_{tâ†’âˆ} P(a_t = a*) = 1
```

**NÃºmero esperado de exploraÃ§Ãµes atÃ© tempo T:**
```
E[# exploraÃ§Ãµes] â‰ˆ âˆ«_1^T 1/âˆšt dt = 2âˆšT - 2 â‰ˆ O(âˆšT)
```

---

## ğŸ“Š MÃ©tricas da IA

### MÃ©tricas em Tempo Real

O sistema registra e exibe as seguintes mÃ©tricas apÃ³s cada interaÃ§Ã£o:

#### 1. **Valores Q (Q-values)**

**DefiniÃ§Ã£o:** Estimativa do valor esperado de cada aÃ§Ã£o.

```
Q(a) â‰ˆ E[R | aÃ§Ã£o = a]
```

**VisualizaÃ§Ã£o no Console:**
```
ğŸ“ˆ Q-Values (Valor estimado de cada aÃ§Ã£o):
  AÃ§Ã£o 0: 0.2100 â–ˆâ–ˆâ–ˆâ–ˆ
  AÃ§Ã£o 1: 0.3200 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  AÃ§Ã£o 2: 0.5400 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  AÃ§Ã£o 3: 0.7100 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  AÃ§Ã£o 4: 0.8500 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â­
  AÃ§Ã£o 5: 0.6800 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  AÃ§Ã£o 6: 0.5200 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  AÃ§Ã£o 7: 0.3800 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  AÃ§Ã£o 8: 0.2500 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  AÃ§Ã£o 9: 0.1800 â–ˆâ–ˆâ–ˆ
```

**InterpretaÃ§Ã£o:**
- Barras longas = aÃ§Ãµes mais valiosas
- â­ = aÃ§Ã£o Ã³tima atual (max Q-value)
- Q-values convergem para valores verdadeiros com T â†’ âˆ

#### 2. **Contagem de AÃ§Ãµes**

**DefiniÃ§Ã£o:** NÃºmero de vezes que cada aÃ§Ã£o foi selecionada.

```
N(a) = nÃºmero de vezes que aÃ§Ã£o a foi escolhida
```

**VisualizaÃ§Ã£o:**
```
ğŸ”¢ Contagem de cada aÃ§Ã£o:
  AÃ§Ã£o 0: 3 vezes (5.0%)
  AÃ§Ã£o 1: 5 vezes (8.3%)
  AÃ§Ã£o 2: 8 vezes (13.3%)
  AÃ§Ã£o 3: 12 vezes (20.0%)
  AÃ§Ã£o 4: 18 vezes (30.0%) â­
  AÃ§Ã£o 5: 7 vezes (11.7%)
  AÃ§Ã£o 6: 4 vezes (6.7%)
  AÃ§Ã£o 7: 2 vezes (3.3%)
  AÃ§Ã£o 8: 1 vez (1.7%)
  AÃ§Ã£o 9: 0 vezes (0.0%)
```

**InterpretaÃ§Ã£o:**
- DistribuiÃ§Ã£o deve concentrar em torno da aÃ§Ã£o Ã³tima
- AÃ§Ãµes exploradas inicialmente, depois focadas na melhor

#### 3. **Regret (Arrependimento)**

**DefiniÃ§Ã£o:** Perda acumulada por nÃ£o escolher sempre a aÃ§Ã£o Ã³tima.

```
L_T = âˆ‘_{t=1}^{T} [Q(a*) - Q(a_t)]
```

**VisualizaÃ§Ã£o:**
```
ğŸ“‰ Regret acumulado: 1.2345
ğŸ“‰ Regret mÃ©dio: 0.0823
```

**InterpretaÃ§Ã£o:**
- Regret baixo = aprendizado eficiente
- Regret mÃ©dio deve diminuir com o tempo
- Crescimento O(log T) Ã© Ã³timo

**GrÃ¡fico esperado:**
```
Regret
  â”‚     
  â”‚    â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  (log T)
  â”‚   â•±
  â”‚  â•±
  â”‚ â•±
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tentativas
```

#### 4. **AÃ§Ã£o Ã“tima**

**DefiniÃ§Ã£o:** AÃ§Ã£o com maior Q-value estimado.

```
a* = argmax_a Q(a)
```

**VisualizaÃ§Ã£o:**
```
â­ AÃ§Ã£o Ã³tima atual: 4
â­ Vezes escolhida: 18 (30.0%)
```

**InterpretaÃ§Ã£o:**
- Deve convergir para a aÃ§Ã£o verdadeiramente Ã³tima
- % deve aumentar com o tempo (mais exploitaÃ§Ã£o)

#### 5. **Recompensas**

**VisualizaÃ§Ã£o:**
```
ğŸ’° Recompensa total: 10.45
ğŸ’° Recompensa mÃ©dia: 0.6967
ğŸ’° Recompensa esperada: 0.7124
```

**FÃ³rmulas:**
```
Recompensa total = âˆ‘_{t=1}^{T} r_t
Recompensa mÃ©dia = (âˆ‘r_t) / T
Recompensa esperada = âˆ‘ Ï€(a) Ã— Q(a)
```

**InterpretaÃ§Ã£o:**
- Recompensa mÃ©dia deve aumentar com aprendizado
- Deve convergir para recompensa da aÃ§Ã£o Ã³tima

#### 6. **ExploraÃ§Ã£o vs ExploitaÃ§Ã£o**

**VisualizaÃ§Ã£o:**
```
ğŸ² DistribuiÃ§Ã£o ExploraÃ§Ã£o/ExploitaÃ§Ã£o:
  ExploraÃ§Ã£o: 24 (40.0%)
  ExploitaÃ§Ã£o: 36 (60.0%)
```

**EvoluÃ§Ã£o temporal esperada:**
```
% ExploitaÃ§Ã£o
  â”‚
100â”‚              â•±â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚            â•±
 50â”‚          â•±
   â”‚        â•±
  0â”‚â”€â”€â”€â”€â”€â”€â•±
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tentativas
```

#### 7. **DistribuiÃ§Ã£o Binomial**

**VisualizaÃ§Ã£o:**
```
ğŸ“Š DistribuiÃ§Ã£o Binomial (ExploitaÃ§Ã£o):
  p = 0.6000
  n = 60
  MÃ©dia = 36.00
  VariÃ¢ncia = 14.4000
  Desvio PadrÃ£o = 3.7947
```

**InterpretaÃ§Ã£o:**
- `p` aumenta com o tempo (mais exploitaÃ§Ã£o)
- VariÃ¢ncia indica incerteza
- IC 95%: `[Î¼ - 1.96Ïƒ, Î¼ + 1.96Ïƒ]`

#### 8. **Epsilon Atual**

**VisualizaÃ§Ã£o:**
```
ğŸšï¸  Epsilon atual: 0.1291
ğŸšï¸  Decaimento: Îµ(t) = 1/âˆš61 = 0.1291
```

**Valores tÃ­picos:**
```
t = 1:    Îµ = 0.7071
t = 10:   Îµ = 0.3015
t = 50:   Îµ = 0.1400
t = 100:  Îµ = 0.0995
t = 500:  Îµ = 0.0447
```

---

## ğŸ’» ImplementaÃ§Ã£o

### Estrutura da Classe `EpsilonGreedyBandit`

```javascript
class EpsilonGreedyBandit {
    constructor() {
        this.nArms = 10;                          // NÃºmero de aÃ§Ãµes
        this.Q = new Array(10).fill(0);          // Q-values
        this.N = new Array(10).fill(0);          // Contadores
        this.trial = 0;                          // Tentativa atual
        this.exploreCount = 0;                   // Contador exploraÃ§Ã£o
        this.exploitCount = 0;                   // Contador exploitaÃ§Ã£o
        this.regret = 0;                         // Regret acumulado
        this.totalReward = 0;                    // Recompensa total
    }
    
    getEpsilon() {
        return this.trial === 0 ? 1.0 : 1.0 / Math.sqrt(this.trial + 1);
    }
    
    selectAction(ballY) {
        this.trial++;
        const epsilon = this.getEpsilon();
        
        if (Math.random() < epsilon) {
            // EXPLORAÃ‡ÃƒO
            return Math.floor(Math.random() * this.nArms);
        } else {
            // EXPLOITAÃ‡ÃƒO
            return this.Q.indexOf(Math.max(...this.Q));
        }
    }
    
    updateReward(action, reward) {
        this.N[action]++;
        const alpha = 1 / this.N[action];
        this.Q[action] += alpha * (reward - this.Q[action]);
        this.totalReward += reward;
        
        // Calcular regret
        const optimalReward = Math.max(...this.Q);
        this.regret += Math.max(0, optimalReward - this.Q[action]);
    }
}
```

### IntegraÃ§Ã£o com o Jogo

```javascript
let bandit = new EpsilonGreedyBandit();
let currentAction = null;

function moveBotEpsilonGreedy() {
    // Decidir aÃ§Ã£o quando bola se aproxima
    if (ball.x > 400 && currentAction === null) {
        currentAction = bandit.selectAction(ball.y);
    }
    
    if (currentAction !== null) {
        const targetY = bandit.getTargetY(currentAction);
        // Mover raquete para targetY...
    }
}

function hitBotPaddle() {
    // Calcular recompensa
    const distance = Math.abs(ball.y - bot.y);
    const reward = 1.0 / (1.0 + distance / 50);
    
    // Atualizar bandit
    bandit.updateReward(currentAction, reward);
    currentAction = null;
}
```

---

## ğŸ“ˆ AnÃ¡lise de ConvergÃªncia

### CondiÃ§Ãµes de ConvergÃªncia

Para que o Epsilon-Greedy convirja para a aÃ§Ã£o Ã³tima:

1. **ExploraÃ§Ã£o infinita:**
```
âˆ‘_{t=1}^{âˆ} Îµ(t) = âˆ
```

2. **ExploraÃ§Ã£o decrescente:**
```
âˆ‘_{t=1}^{âˆ} ÎµÂ²(t) < âˆ
```

**VerificaÃ§Ã£o para Îµ(t) = 1/âˆšt:**

```
âˆ‘ 1/âˆšt â‰ˆ âˆ« 1/âˆšt dt = 2âˆšt â†’ âˆ  âœ“

âˆ‘ (1/âˆšt)Â² = âˆ‘ 1/t â‰ˆ âˆ« 1/t dt = ln(t) < âˆ  âœ“
```

### Taxa de ConvergÃªncia

**Limite superior do regret:**
```
E[L_T] â‰¤ c Ã— log(T) + o(log T)
```

**Onde `c` depende:**
- Do gap entre aÃ§Ã£o Ã³tima e subÃ³timas: `Î”_a = Q(a*) - Q(a)`
- Do schedule de epsilon

**Para Îµ(t) = 1/âˆšt:**
```
E[L_T] = O(âˆšT Ã— log T)
```

### MÃ©tricas de ConvergÃªncia

**1. Taxa de escolha da aÃ§Ã£o Ã³tima:**
```
P(a_t = a*) â†’ 1  quando t â†’ âˆ
```

**2. Q-value da aÃ§Ã£o Ã³tima:**
```
Q_t(a*) â†’ Q*(a*)  quando t â†’ âˆ
```

**3. Regret mÃ©dio:**
```
LÌ„_T = L_T / T â†’ 0  quando T â†’ âˆ
```

---

## ğŸ§ª Experimentos

### Experimento 1: ConvergÃªncia dos Q-values

**Objetivo:** Verificar se Q-values convergem para valores verdadeiros.

**Protocolo:**
1. Executar 100 tentativas
2. Registrar Q-values a cada 10 tentativas
3. Plotar evoluÃ§Ã£o temporal

**Resultado esperado:**
```
Q-value
  â”‚
1.0â”‚        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Q*(a*) = 0.85
   â”‚      â•±
0.5â”‚    â•±
   â”‚  â•±
0.0â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tentativas
   0   25   50   75   100
```

### Experimento 2: Regret vs Tentativas

**Objetivo:** Verificar crescimento logarÃ­tmico do regret.

**Protocolo:**
1. Executar 500 tentativas
2. Registrar regret acumulado
3. Plotar em escala log-log

**Resultado esperado:**
```
log(Regret)
  â”‚     
  â”‚    â•±â”€â”€â”€â”€â”€â”€  (slope â‰ˆ 0.5)
  â”‚   â•±
  â”‚  â•±
  â”‚ â•±
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ log(Tentativas)
```

### Experimento 3: Taxa de ExploraÃ§Ã£o

**Objetivo:** Verificar decaimento de epsilon.

**Protocolo:**
1. Registrar % de exploraÃ§Ã£o em janelas de 10 tentativas
2. Comparar com Îµ(t) teÃ³rico

**Resultado esperado:**
```
% ExploraÃ§Ã£o
  â”‚
100â”‚â•²
   â”‚ â•²_
 50â”‚   â•²___
   â”‚       â•²____
  0â”‚            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tentativas
   0    50   100   150  200
```

### Experimento 4: ComparaÃ§Ã£o de Schedules

**Objetivo:** Comparar diferentes schedules de epsilon.

**Schedules testados:**
```javascript
Îµ1(t) = 1/âˆšt         // PadrÃ£o
Îµ2(t) = 1/t          // Decai mais rÃ¡pido
Îµ3(t) = 1/log(t+2)   // Decai mais devagar
Îµ4(t) = 0.1          // Constante
```

**MÃ©trica:** Regret acumulado em 200 tentativas.

**Resultado esperado:**
- `Îµ1`: Melhor balanÃ§o exploraÃ§Ã£o/exploitaÃ§Ã£o
- `Îµ2`: ConvergÃªncia prematura
- `Îµ3`: ExploraÃ§Ã£o excessiva
- `Îµ4`: Regret linear

---

## ğŸš€ InstalaÃ§Ã£o

### MÃ©todo 1: Abrir Direto

```bash
# 1. Baixe o arquivo index.html
# 2. Abra em um navegador moderno
```

### MÃ©todo 2: Servidor Local

```bash
# Python
python3 -m http.server 8000

# Node.js
npx http-server

# Acesse: http://localhost:8000
```

### Requisitos

- **Navegador:** Chrome 90+, Firefox 88+, Safari 14+
- **JavaScript:** Habilitado
- **Console:** Aberto (F12) para ver mÃ©tricas

---

## ğŸ® Como Usar

### 1. Iniciar o Jogo

1. Abra o arquivo HTML
2. Pressione **F12** para abrir o Console
3. Clique no botÃ£o **"COMEÃ‡AR"**

### 2. Observar MÃ©tricas

As mÃ©tricas aparecem no console apÃ³s cada interaÃ§Ã£o da IA com a bola:

```javascript
// Exemplo de saÃ­da
ğŸ” [Tentativa 15] EXPLORAÃ‡ÃƒO (Îµ=0.2500)
======================================================================
ğŸ“Š MÃ‰TRICAS - Tentativa 15
======================================================================
ğŸ¯ AÃ§Ã£o escolhida: 4 (PosiÃ§Ã£o Y: 240-300)
ğŸ Recompensa: 0.8235
...
```

### 3. Analisar Aprendizado

**Indicadores de aprendizado bem-sucedido:**

âœ… Q-values convergem (barras param de oscilar)  
âœ… AÃ§Ã£o Ã³tima escolhida com frequÃªncia crescente  
âœ… Regret mÃ©dio diminui  
âœ… Recompensa mÃ©dia aumenta  
âœ… % ExploitaÃ§Ã£o aumenta (>70% apÃ³s 50 tentativas)

### 4. Controles do Jogador

- **Desktop:** `â†‘` `â†“` ou `W` `S`
- **Mobile:** BotÃµes na tela

---

## ğŸ“š ReferÃªncias TeÃ³ricas

### Papers Fundamentais

1. **Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002)**  
   *"Finite-time Analysis of the Multiarmed Bandit Problem"*  
   Machine Learning, 47(2-3), 235-256.

2. **Sutton, R. S., & Barto, A. G. (2018)**  
   *"Reinforcement Learning: An Introduction"* (2nd ed.)  
   MIT Press. Chapter 2: Multi-armed Bandits.

3. **Lattimore, T., & SzepesvÃ¡ri, C. (2020)**  
   *"Bandit Algorithms"*  
   Cambridge University Press.

### Teoremas Relevantes

**Teorema 1 (ConvergÃªncia):**  
Se Îµ(t) satisfaz as condiÃ§Ãµes de Robbins-Monro, entÃ£o:
```
lim_{tâ†’âˆ} P(a_t = a*) = 1
```

**Teorema 2 (Regret LogarÃ­tmico):**  
Para qualquer algoritmo com exploraÃ§Ã£o decrescente:
```
E[L_T] = Î©(log T)
```

**Teorema 3 (Limite Superior):**  
Para Îµ-greedy com Îµ(t) = t^(-Î±), Î± âˆˆ (1/2, 1):
```
E[L_T] = O(T^(1-Î±) log T)
```

### Links Ãšteis

- **Sutton & Barto (online):** http://incompleteideas.net/book/
- **Multi-Armed Bandits Tutorial:** https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/
- **Epsilon-Greedy Explained:** https://www.cs.mcgill.ca/~vkules/bandits.pdf

---

## ğŸ› ï¸ Desenvolvimento

### Modificar Schedule de Epsilon

```javascript
class EpsilonGreedyBandit {
    getEpsilon() {
        // OpÃ§Ã£o 1: 1/âˆšt (padrÃ£o)
        return 1.0 / Math.sqrt(this.trial + 1);
        
        // OpÃ§Ã£o 2: 1/t (mais agressivo)
        // return 1.0 / (this.trial + 1);
        
        // OpÃ§Ã£o 3: 1/log(t) (mais conservador)
        // return 1.0 / Math.log(this.trial + 2);
        
        // OpÃ§Ã£o 4: Constante
        // return 0.1;
    }
}
```

### Adicionar Logging CSV

```javascript
updateReward(action, reward) {
    // ... cÃ³digo existente ...
    
    // Exportar para CSV
    const csvLine = `${this.trial},${action},${reward},${this.Q[action]},${this.regret}\n`;
    console.log('CSV:', csvLine);
}
```

### Visualizar em Tempo Real

```javascript
// Usar bibliotecas como Chart.js ou Plotly
const ctx = document.getElementById('qvalues-chart');
new Chart(ctx, {
    type: 'bar',
    data: {
        labels: ['AÃ§Ã£o 0', 'AÃ§Ã£o 1', ..., 'AÃ§Ã£o 9'],
        datasets: [{
            label: 'Q-values',
            data: this.Q
        }]
    }
});
```

---

## ğŸ“Š AnÃ¡lise EstatÃ­stica AvanÃ§ada

### Intervalo de ConfianÃ§a para Q-values

Usando **Teorema Central do Limite**:

```
Q(a) ~ Normal(Î¼, ÏƒÂ²/N(a))
```

**IC 95%:**
```
[Q(a) - 1.96 Ã— Ïƒ/âˆšN(a), Q(a) + 1.96 Ã— Ïƒ/âˆšN(a)]
```
